{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7c0636-d758-401a-a459-a11c7b3c683a",
   "metadata": {},
   "source": [
    "# Markov Reward Process (MRP) Explanation\n",
    "\n",
    "## Introduction\n",
    "A **Markov Reward Process (MRP)** is a mathematical framework used to model sequential decision-making problems where transitions between states occur probabilistically, and each state provides a numerical reward. It is a simplified version of a **Markov Decision Process (MDP)** that does not involve actions.\n",
    "\n",
    "An MRP is formally defined as a tuple **(S, P, R, γ)**:\n",
    "- **S**: A finite set of states.\n",
    "- **P**: A state transition probability matrix where \\( P(s' | s) \\) represents the probability of transitioning from state \\( s \\) to state \\( s' \\).\n",
    "- **R**: A reward function mapping each state to a real-valued reward \\( R(s) \\).\n",
    "- **$\\gamma$**: A discount factor $$ \\gamma \\in [0,1] $$ that determines the importance of future rewards.\n",
    "---\n",
    "\n",
    "## MRP Bellman Equation\n",
    "\n",
    "$$\n",
    "V(s) = R(s) + \\gamma \\sum_{s' \\in S} P(s' \\mid s) V(s')\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(V(s)\\) is the expected **long-term reward** when starting from state \\(s\\).\n",
    "- \\(R(s)\\) is the **immediate reward** received in state \\(s\\).\n",
    "- \\(\\gamma\\) is the **discount factor**, where \\(0 \\leq \\gamma \\leq 1\\), which determines the importance of future rewards.\n",
    "- \\(P(s' \\mid s)\\) is the **transition probability**, representing the probability of moving from state \\(s\\) to \\(s'\\).\n",
    "\n",
    "### Matrix Representation\n",
    "\n",
    "For a finite set of states $(S = \\{s_1, s_2, \\dots, s_n\\})$, we can represent the Bellman equation in **matrix form**:\n",
    "\n",
    "$$\n",
    "V = (I - \\gamma P)^{-1} R\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(V\\) is the column vector of state values: \\(V = [V(s_1), V(s_2), \\dots, V(s_n)]^T\\).\n",
    "- \\(R\\) is the column vector of rewards: \\(R = [R(s_1), R(s_2), \\dots, R(s_n)]^T\\).\n",
    "- \\(P\\) is the **state transition matrix**, where each entry \\(P_{ij} = P(s_j \\mid s_i)\\).\n",
    "- \\(I\\) is the identity matrix.\n",
    "\n",
    "\n",
    "\n",
    "## Code Explanation\n",
    "\n",
    "The provided Python code implements an MRP and computes the **state value function \\( V(s) \\)** using the Bellman equation.\n",
    "\n",
    "### **1. Defining the MRP Components**\n",
    "```python\n",
    "states = ['A', 'B', 'C', 'D']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb602735-af1c-4c4d-aab5-8cec547cb455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(A) = 15.90\n",
      "V(B) = 16.11\n",
      "V(C) = 17.00\n",
      "V(D) = 20.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "states = ['A', 'B', 'C', 'D']\n",
    "\n",
    "transition_matrix = np.array([\n",
    "    [0.0, 0.5, 0.5, 0.0],  # A -> {B: 0.5, C: 0.5}\n",
    "    [0.0, 0.0, 0.7, 0.3],  # B -> {C: 0.7, D: 0.3}\n",
    "    [0.0, 0.0, 0.0, 1.0],  # C -> {D: 1.0}\n",
    "    [0.0, 0.0, 0.0, 1.0]   # D -> {D: 1.0} (terminal state)\n",
    "])\n",
    "\n",
    "# R(s)\n",
    "rewards = np.array([1, 0, -1, 2])  # A: +1, B: 0, C: -1, D: +2\n",
    "\n",
    "# discount rate\n",
    "gamma = 0.9\n",
    "\n",
    "# bellman equation\n",
    "I = np.eye(len(states))\n",
    "V = np.linalg.inv(I - gamma * transition_matrix) @ rewards\n",
    "\n",
    "for state, value in zip(states, V):\n",
    "    print(f\"V({state}) = {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8570d-76ad-43f3-8bdd-e9f3945790ca",
   "metadata": {},
   "source": [
    "# **Markov Decision Process (MDP) Explanation**\n",
    "\n",
    "## **Introduction**\n",
    "A **Markov Decision Process (MDP)** is a mathematical framework used in reinforcement learning to model decision-making in environments where outcomes are partly random and partly under the control of an agent.\n",
    "\n",
    "An MDP is formally defined as a tuple **(S, A, P, R, γ)**:\n",
    "- **S**: A finite set of states.\n",
    "- **A**: A finite set of actions available to the agent.\n",
    "- **P(s' | s, a)**: A transition probability matrix that represents the probability of transitioning from state \\( s \\) to \\( s' \\) given action \\( a \\).\n",
    "- **R(s, a)**: A reward function that maps each state-action pair to a numerical reward.\n",
    "- **$\\gamma$**: A discount factor $( \\gamma \\in [0,1] )$ that determines the importance of future rewards.\n",
    "\n",
    "The goal of an MDP is to find an **optimal policy** $( \\pi^*(s) )$ that maximizes the **expected cumulative reward** over time.\n",
    "\n",
    "---\n",
    "\n",
    "## Bellman Optimality Equation for MDP\n",
    "\n",
    "In a Markov Decision Process (MDP) without a given policy, we are interested in finding the **optimal state value function** \\(V^*(s)\\). This function represents the maximum expected cumulative reward achievable from state \\(s\\) by choosing the best possible actions. The Bellman Optimality Equation is given by:\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a \\in A} \\left[ R(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, a) V^*(s') \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $(V^*(s))$ is the optimal expected **long-term reward** from state \\(s\\).\n",
    "- $(R(s, a))$ is the **immediate reward** received after taking action \\(a\\) in state \\(s\\).\n",
    "- $(\\gamma)$ is the **discount factor**, with \\(0 \\leq \\gamma \\leq 1\\), which scales the importance of future rewards.\n",
    "- $(P(s' \\mid s, a))$ is the **transition probability**, representing the probability of moving to state \\(s'\\) from state \\(s\\) after taking action \\(a\\).\n",
    "- $(A)$ is the set of all available actions.\n",
    "\n",
    "\n",
    "\n",
    "## **Code Explanation**\n",
    "The provided Python code implements an MDP and solves it using the **Value Iteration algorithm**, which finds the optimal **state value function** \\( V^*(s) \\) and the corresponding **optimal policy** $( \\pi^*(s) )$.\n",
    "\n",
    "### **1. Defining the MDP Components**\n",
    "```python\n",
    "states = [\"A\", \"B\", \"C\", \"D\"]\n",
    "actions = [\"left\", \"right\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afe67a7-72cc-4b11-bc8d-7a0128c6bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function (V):\n",
      "V(A) = 5.23\n",
      "V(B) = 4.70\n",
      "V(C) = 3.00\n",
      "V(D) = 0.00\n",
      "\n",
      "Optimal Policy:\n",
      "π(A) = right\n",
      "π(B) = right\n",
      "π(C) = right\n",
      "π(D) = None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "states = [\"A\", \"B\", \"C\", \"D\"]\n",
    "actions = [\"left\", \"right\"]\n",
    "\n",
    "# P(s' | s, a)\n",
    "transition_probs = {\n",
    "    \"A\": {\"left\": {\"A\": 1.0}, \"right\": {\"B\": 1.0}},\n",
    "    \"B\": {\"left\": {\"A\": 0.5, \"B\": 0.5}, \"right\": {\"C\": 1.0}},\n",
    "    \"C\": {\"left\": {\"B\": 1.0}, \"right\": {\"D\": 1.0}},\n",
    "    \"D\": {\"left\": {\"C\": 1.0}, \"right\": {\"D\": 1.0}},  # 종료 상태\n",
    "}\n",
    "\n",
    "# R(s, a)\n",
    "rewards = {\n",
    "    \"A\": {\"left\": 0, \"right\": 1},\n",
    "    \"B\": {\"left\": -1, \"right\": 2},\n",
    "    \"C\": {\"left\": -2, \"right\": 3},\n",
    "    \"D\": {\"left\": 0, \"right\": 0},\n",
    "}\n",
    "\n",
    "# (Gamma)\n",
    "gamma = 0.9\n",
    "\n",
    "# Ininital Value of State\n",
    "V = {s: 0 for s in states}\n",
    "\n",
    "# **Value Iteration Algorithm**\n",
    "tolerance = 1e-6 \n",
    "while True:\n",
    "    delta = 0\n",
    "    new_V = V.copy()\n",
    "    \n",
    "    for s in states:\n",
    "        if s == \"D\":  # 종료 상태는 그대로 유지\n",
    "            continue\n",
    "\n",
    "        # 가능한 모든 행동에 대해 기대 가치 계산\n",
    "        action_values = []\n",
    "        for a in actions:\n",
    "            value = sum(p * (rewards[s][a] + gamma * V[s_next])\n",
    "                        for s_next, p in transition_probs[s][a].items())\n",
    "            action_values.append(value)\n",
    "\n",
    "        # 새로운 가치 함수 업데이트\n",
    "        new_V[s] = max(action_values)\n",
    "        delta = max(delta, abs(new_V[s] - V[s]))\n",
    "\n",
    "    V = new_V  # 가치 함수 업데이트\n",
    "\n",
    "    if delta < tolerance:  # 수렴 조건\n",
    "        break\n",
    "\n",
    "# 최적 정책 추출\n",
    "policy = {}\n",
    "for s in states:\n",
    "    if s == \"D\":  # 종료 상태는 정책이 필요 없음\n",
    "        policy[s] = None\n",
    "        continue\n",
    "    \n",
    "    best_action = None\n",
    "    best_value = float(\"-inf\")\n",
    "    \n",
    "    for a in actions:\n",
    "        value = sum(p * (rewards[s][a] + gamma * V[s_next])\n",
    "                    for s_next, p in transition_probs[s][a].items())\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = a\n",
    "    \n",
    "    policy[s] = best_action\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Optimal Value Function (V):\")\n",
    "for state in V:\n",
    "    print(f\"V({state}) = {V[state]:.2f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for state in policy:\n",
    "    print(f\"π({state}) = {policy[state]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3129e7d-14a0-4177-9f02-3c9a18dad580",
   "metadata": {},
   "source": [
    "# Markov Decision Process + Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d08d5-8a7b-4457-9a7d-46bcc300e904",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "A **Markov Decision Process (MDP)** is a mathematical framework used in reinforcement learning to model decision-making in environments where outcomes are partly random and partly under the control of an agent.\n",
    "\n",
    "An MDP is formally defined as a tuple **(S, A, P, R, γ)**:\n",
    "- **S**: A finite set of states.\n",
    "- **A**: A finite set of actions available to the agent.\n",
    "- **P(s' | s, a)**: A transition probability matrix that represents the probability of transitioning from state \\( s \\) to \\( s' \\) given action \\( a \\).\n",
    "- **R(s, a)**: A reward function that maps each state-action pair to a numerical reward.\n",
    "- **\\(\\gamma\\)**: A discount factor \\( (\\gamma \\in [0,1]) \\) that determines the importance of future rewards.\n",
    "\n",
    "The goal of an MDP is to find an **optimal policy** \\( \\pi^*(s) \\) that maximizes the **expected cumulative reward** over time.\n",
    "\n",
    "---\n",
    "\n",
    "## Bellman Equation for MDP with a Given Policy\n",
    "\n",
    "When a policy $( \\pi )$ is provided, the **state value function** $( V^\\pi(s) )$ gives the expected cumulative reward when starting from state ( s ) and following the policy $( \\pi )$. The Bellman Equation for a given policy is defined as:\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, a) V^\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $( V^\\pi(s) )$ is the expected **long-term reward** from state \\( s \\) when following policy \\( \\pi \\).\n",
    "- $( \\pi(a \\mid s) )$ is the probability of taking action \\( a \\) in state \\( s \\) under policy \\( \\pi \\).\n",
    "- $( R(s, a) )$ is the **immediate reward** received after taking action \\( a \\) in state \\( s \\).\n",
    "- $( \\gamma )$ is the **discount factor**, with \\( 0 \\leq \\gamma \\leq 1 \\), which scales the importance of future rewards.\n",
    "- $( P(s' \\mid s, a) )$ is the **transition probability**, representing the probability of moving to state \\( s' \\) from state \\( s \\) after taking action \\( a )$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Explanation**\n",
    "The following Python code implements a simple MDP and evaluates a given policy using the **Value Iteration algorithm**. This process computes the state value function \\( V^\\pi(s) \\) and can be extended to derive the corresponding policy \\( \\pi(s) \\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08980237-7daa-4668-b4c4-f8720d6bc6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRP Transition Matrix (P^π):\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "MRP Reward Vector (R^π):\n",
      "[1. 2. 3. 0.]\n",
      "\n",
      "State Value Function under the Policy (V^π):\n",
      "V^π(A) = 5.23\n",
      "V^π(B) = 4.70\n",
      "V^π(C) = 3.00\n",
      "V^π(D) = 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define states and actions\n",
    "states = [\"A\", \"B\", \"C\", \"D\"]\n",
    "actions = [\"left\", \"right\"]\n",
    "\n",
    "# Transition probabilities P(s' | s, a) for the MDP\n",
    "transition_probs = {\n",
    "    \"A\": {\"left\": {\"A\": 1.0}, \"right\": {\"B\": 1.0}},\n",
    "    \"B\": {\"left\": {\"A\": 0.5, \"B\": 0.5}, \"right\": {\"C\": 1.0}},\n",
    "    \"C\": {\"left\": {\"B\": 1.0}, \"right\": {\"D\": 1.0}},\n",
    "    \"D\": {\"left\": {\"C\": 1.0}, \"right\": {\"D\": 1.0}},  # terminal state\n",
    "}\n",
    "\n",
    "# Reward function R(s, a)\n",
    "rewards = {\n",
    "    \"A\": {\"left\": 0, \"right\": 1},\n",
    "    \"B\": {\"left\": -1, \"right\": 2},\n",
    "    \"C\": {\"left\": -2, \"right\": 3},\n",
    "    \"D\": {\"left\": 0, \"right\": 0},  # terminal state\n",
    "}\n",
    "\n",
    "# Discount factor (gamma)\n",
    "gamma = 0.9\n",
    "\n",
    "# Define a policy π(s) (e.g., always choose 'right')\n",
    "policy = {\n",
    "    \"A\": \"right\",\n",
    "    \"B\": \"right\",\n",
    "    \"C\": \"right\",\n",
    "    \"D\": None  # No action for the terminal state\n",
    "}\n",
    "\n",
    "# Convert the MDP to an MRP: Compute new P^π and R^π\n",
    "P_pi = np.zeros((len(states), len(states)))\n",
    "R_pi = np.zeros(len(states))\n",
    "\n",
    "state_idx = {s: i for i, s in enumerate(states)}  # Map state names to indices\n",
    "\n",
    "for s in states:\n",
    "    if s == \"D\":  # Terminal state remains unchanged\n",
    "        P_pi[state_idx[s], state_idx[s]] = 1.0\n",
    "        R_pi[state_idx[s]] = 0\n",
    "        continue\n",
    "\n",
    "    a = policy[s]  # Selected action\n",
    "    for s_next, prob in transition_probs[s][a].items():\n",
    "        P_pi[state_idx[s], state_idx[s_next]] = prob\n",
    "\n",
    "    R_pi[state_idx[s]] = rewards[s][a]\n",
    "\n",
    "# Solve the MRP Bellman equation: V^π = (I - γP^π)^(-1) R^π\n",
    "I = np.eye(len(states))\n",
    "V_pi = np.linalg.inv(I - gamma * P_pi) @ R_pi\n",
    "\n",
    "# Print the results\n",
    "print(\"MRP Transition Matrix (P^π):\")\n",
    "print(P_pi)\n",
    "\n",
    "print(\"\\nMRP Reward Vector (R^π):\")\n",
    "print(R_pi)\n",
    "\n",
    "print(\"\\nState Value Function under the Policy (V^π):\")\n",
    "for s, v in zip(states, V_pi):\n",
    "    print(f\"V^π({s}) = {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02697b-1421-41ec-94f9-4623fe6f1ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
